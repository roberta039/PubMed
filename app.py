import streamlit as st
from Bio import Entrez
from groq import Groq

# Configurare paginÄƒ
st.set_page_config(page_title="Medical Assistant", page_icon="ğŸ©º", layout="wide")

# --- 1. SECRETS MANAGEMENT ---
try:
    if "GROQ_API_KEY" in st.secrets:
        api_key = st.secrets["GROQ_API_KEY"]
    else:
        st.error("LipseÈ™te GROQ_API_KEY din secrets.")
        st.stop()
        
    if "EMAIL_ADRESS" in st.secrets:
        email_address = st.secrets["EMAIL_ADRESS"]
    else:
        email_address = st.secrets.get("EMAIL_ADDRESS", "email@test.com")
        
except FileNotFoundError:
    st.error("ConfigureazÄƒ secrets.toml!")
    st.stop()

# IniÈ›ializare client Groq
client = Groq(api_key=api_key)

# --- 2. FUNCÈšII ---

def search_pubmed(query, email, max_results=5):
    """CautÄƒ pe PubMed direct cu termenul introdus"""
    Entrez.email = email
    try:
        # CÄƒutare
        handle = Entrez.esearch(db="pubmed", term=query, retmax=max_results, sort="relevance")
        record = Entrez.read(handle)
        handle.close()
        
        id_list = record["IdList"]
        st.write(f"â„¹ï¸ S-au gÄƒsit {len(id_list)} studii pentru: *{query}*")

        if not id_list:
            return None

        # DescÄƒrcare detalii
        handle = Entrez.efetch(db="pubmed", id=id_list, rettype="medline", retmode="text")
        articles_text = handle.read()
        handle.close()
        return articles_text
    except Exception as e:
        st.error(f"Eroare PubMed: {e}")
        return None

def generate_answer(query, context):
    """GenereazÄƒ rÄƒspunsul final folosind Llama 3.3"""
    prompt = f"""
    EÈ™ti un asistent medical expert. Sarcina ta este sÄƒ sintetizezi informaÈ›ia din studiile de mai jos.
    
    Ãntrebarea utilizatorului: {query}
    
    Context (Studii PubMed):
    {context}
    
    InstrucÈ›iuni:
    1. RÄƒspunde Ã®n LIMBA ROMÃ‚NÄ‚.
    2. FoloseÈ™te doar informaÈ›iile din context.
    3. DacÄƒ studiile nu sunt relevante, spune asta.
    """
    
    try:
        # FOLOSIM NOUA VERSIUNE DE MODEL: llama-3.3-70b-versatile
        completion = client.chat.completions.create(
            model="llama-3.3-70b-versatile", 
            messages=[
                {"role": "system", "content": "EÈ™ti un medic cercetÄƒtor care rÄƒspunde Ã®n limba romÃ¢nÄƒ."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
        )
        return completion.choices[0].message.content
    except Exception as e:
        return f"Eroare AI: {e}"

# --- 3. INTERFAÈšA ---
st.title("ğŸ©º PubMed AI Assistant (Groq Free)")
st.markdown("""
Acest asistent cautÄƒ pe PubMed È™i sintetizeazÄƒ rezultatele.
**Sfat:** Pentru cele mai bune rezultate, introduceÈ›i termenii de cÄƒutare Ã®n **EnglezÄƒ** (ex: *aspirin side effects*), dar AI-ul va rÄƒspunde Ã®n RomÃ¢nÄƒ.
""")

query = st.text_input("Termen de cÄƒutare (preferabil Ã®n EnglezÄƒ):", placeholder="ex: immunotherapy lung cancer")

if st.button("CautÄƒ"):
    if not query:
        st.warning("Scrie o Ã®ntrebare.")
    else:
        # 1. CÄƒutare directÄƒ
        with st.spinner("CÄƒutÄƒm studii pe PubMed..."):
            pubmed_data = search_pubmed(query, email_address)
            
        # 2. AnalizÄƒ
        if pubmed_data:
            with st.expander("Vezi rezumatele studiilor (EnglezÄƒ)"):
                st.text(pubmed_data)
                
            with st.spinner("Llama 3.3 analizeazÄƒ datele..."):
                ans = generate_answer(query, pubmed_data)
                st.markdown("### RÄƒspuns Sintetizat (RomÃ¢nÄƒ):")
                st.write(ans)
        else:
            st.error("Nu am gÄƒsit studii. ÃncearcÄƒ sÄƒ foloseÈ™ti termeni Ã®n englezÄƒ.")
